{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArmedBanditsEnv():\n",
    "    \"\"\"\n",
    "    num_expt -> number of experiments \n",
    "    num_slots -> available slots that can transmit data based on availablility\n",
    "    p_values -> num_expts x num_slots matrix containing p-values for availability of slot\n",
    "    action -> num_expts x num_slots array denoting the order of checking slots for availability for each expt\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, p_values):\n",
    "        assert len(p_values.shape) == 2\n",
    "        \n",
    "        self.num_slots = p_values.shape[1]\n",
    "        self.num_expts = p_values.shape[0]\n",
    "        self.state = np.zeros((self.num_expts,self.num_slots))\n",
    "        \n",
    "        self.p_values = p_values\n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        # Sample from the specified slot using it's bernoulli distribution\n",
    "        assert (action.shape == (self.num_expts,self.num_slots))\n",
    "        \n",
    "        sampled_state = np.random.binomial(n=1, p=self.p_values)\n",
    "\n",
    "        self.state = sampled_state\n",
    "\n",
    "        cost = np.zeros((1, 1))\n",
    "\n",
    "        for j in range(1):\n",
    "            # Get the relevant actions and their indices for the current experiment\n",
    "            actions = np.array(action[j]) - 1  # Adjust for zero-based indexing\n",
    "            relevant_states = sampled_state[j, actions]\n",
    "            # Find the index of the first occurrence of 1, if any\n",
    "            indices = np.where(relevant_states == 1)[0]\n",
    "            if indices.size > 0:\n",
    "                first_one_index = indices[0]\n",
    "            else:\n",
    "                first_one_index = len(actions)\n",
    "            # Calculate the cost\n",
    "            cost[j] = first_one_index\n",
    "        \n",
    "        # Return a constant state of 0. Our environment has no terminal state\n",
    "        observation, done, info = 0, False, dict()\n",
    "        return observation, cost, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        return 0\n",
    "        \n",
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "    \n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np.random(seed)\n",
    "        return [seed]\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class ArmedBanditsBernoulli(ArmedBanditsEnv):\n",
    "    def __init__(self, num_expts=1, num_slots=5):\n",
    "        self.p_values = np.random.uniform(0, 1, (num_expts, num_slots))\n",
    "        \n",
    "        ArmedBanditsEnv.__init__(self, self.p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [[1 1 0 1]] on Order: [[3 1 2 4]]  gave a cost of: [1.]\n",
      "State: [[0 1 0 0]] on Order: [[3 4 2 1]]  gave a cost of: [2.]\n",
      "State: [[0 0 0 0]] on Order: [[3 2 4 1]]  gave a cost of: [4.]\n",
      "State: [[0 0 0 0]] on Order: [[4 1 3 2]]  gave a cost of: [4.]\n"
     ]
    }
   ],
   "source": [
    "p_values = np.array([[0.1, 0.4, 0.2, 0.3]]) # The p_values for a four-slot channel. Single experiment\n",
    "\n",
    "env = ArmedBanditsEnv(p_values) # Create the environment\n",
    "\n",
    "for i in range(4):\n",
    "    action = np.random.choice(range(1, 5), size=(1,4), replace=False)\n",
    "    _, cost, _, _ = env.step(action)\n",
    "    state = env.state\n",
    "    print(\"State:\",state ,\"on Order:\", action, \" gave a cost of:\",cost[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc_p(prev_p, new_val, n):\n",
    "    return (prev_p*(n-1) + new_val)/n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE GREEDY AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_order(estimates):\n",
    "    \"\"\"\n",
    "    Takes in an array of estimates of num_expts x num_slots and returns the order\n",
    "    of slots with the decreasing estimated p_value for each row. \n",
    "    Breaks ties randomly.\n",
    "    \"\"\"\n",
    "    if estimates.ndim == 1:\n",
    "        # For a 1D array, simply sort and adjust for 1-based indexing\n",
    "        sorted_indices = np.argsort(-estimates) + 1\n",
    "    else:\n",
    "        # For a 2D array, sort along the columns and adjust for 1-based indexing\n",
    "        sorted_indices = np.argsort(-estimates, axis=1) + 1\n",
    "    return sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 1 3]\n",
      "[[2 4 1 3]\n",
      " [1 4 2 3]]\n"
     ]
    }
   ],
   "source": [
    "estimates_1d = np.array([0.1, 0.3, 0.05, 0.2])\n",
    "print(greedy_order(estimates_1d))\n",
    "\n",
    "estimates_2d = np.array([[0.1, 0.3, 0.05, 0.2], [0.4, 0.2, 0.1, 0.3]])\n",
    "print(greedy_order(estimates_2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyAgent:\n",
    "    def __init__(self, estimates):\n",
    "        \"\"\"\n",
    "        Our agent takes as input the initial p_value estimates.\n",
    "        This estimates will be updated incrementally after each \n",
    "        interaction with the environment.\n",
    "        \"\"\"\n",
    "        assert len(estimates.shape) == 2\n",
    "        \n",
    "        self.num_slots = estimates.shape[1]\n",
    "        self.num_expts = estimates.shape[0]\n",
    "        self.estimates = estimates.astype(np.float64)\n",
    "        self.action_count = np.zeros(estimates.shape)\n",
    "        \n",
    "    def get_action(self):\n",
    "        # Our agent is greedy, so there's no need for exploration.\n",
    "        # Our argmax will do just fine for this situation\n",
    "        action = greedy_order(self.estimates)\n",
    "        \n",
    "        # Add a 1 to each action selected in the action count\n",
    "        self.action_count[np.arange(self.num_experiments), action] += 1\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def update_estimates(self, reward, action):\n",
    "        # rew is a matrix with the obtained rewards from our previuos\n",
    "        # action. Use this to update our estimates incrementally\n",
    "        n = self.action_count[np.arange(self.num_experiments), action]\n",
    "        prev_reward_estimates = self.reward_estimates[np.arange(self.num_experiments), action]\n",
    "        \n",
    "        # Update the reward estimates incementally\n",
    "        self.reward_estimates[np.arange(self.num_experiments), action] = inc_avg(prev_reward_estimates,reward,n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bandits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
